Holo's test suite
=================

Holo is tested by running its subcommands on fabricated target/backup/repo
directories and comparing the output and resulting filesystem with the expected
results.

Each test case is in one of the subdirectories, with the directory names sorted
by ascending test complexity (so that when something breaks, it breaks in the
easy tests first and you'll have an easier time figuring out the bug). Each
test case looks like this:

    test/                         <-- this directory
        01-basic/                 <-- the directory for the test setup
            source/               <-- the root directory before running `holo apply`
            target/               <-- the root directory after running `holo apply` (generated by test run)
            scan-output           <-- the expected output of `holo scan`  (minus colors, generated by test run)
            apply-output          <-- the expected output of `holo apply` (minus colors, generated by test run)
            tree                  <-- a dump of the actual files in the target/ directory after running `holo apply` (generated by test run)
            expected-tree         <-- a dump of the expected files in the target/ directory after running `holo apply`
            expected-scan-output  <-- the expected output of `holo scan`  (minus colors)
            expected-apply-output <-- the expected output of `holo apply` (minus colors)

Running the tests
-----------------

To run all the tests, use the make targets `test` or `check` in the top-level
directory or say

    sh test/run_tests.sh

To run just some tests, give the test names (the directory names below the
`test` directory) as arguments:

    sh test/run_tests.sh 01-basic

The test will usually output just a progress display (one line per test case).
If a test fails, the failing parts will be printed on the command line. To flag
test failure when run in Continuous Integration setups, `run_tests.sh` will
exit with non-zero status when one or more test cases fail.

*Warning:* The first step when running a testcase is to `git clean -dXf`, so
all files that are not tracked by Git will be removed. You should therefore
`git add` all files in there before running the testcase.

Writing new testcases
---------------------

There are plenty of files checked into version control for each testcase, but
luckily you can have `run_tests.sh` generate most of them for you. To create a
new testcase, create the new subdirectory below this directory and put in there

* `source/`: This contains the filesystem tree with all the files relevant for
  the test. The test then consists of running `holo scan` and `holo apply` in a
  chroot here and seeing what it does to this filesystem tree.
* Also, a `README.md` explaining the test setup is recommended.

Now run your testcase. It will obviously fail since all the files that it
compares its build results to are missing. But we can use these build results
as basis for the missing files. Copy

    tree            -> expected-tree
    apply-output    -> expected-apply-output
    scan-output     -> expected-scan-output

And the most important step of them all, before checking them into source
control, verify carefully that these files really contain the *expected*
results of the testcase run. When that is done, your testcase should now pass.
Or not, if the code needs fixing. ;)
